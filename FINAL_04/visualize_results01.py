import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import os
import sys
import seaborn as sns
from sklearn.metrics import confusion_matrix

# --- Configuration for specific DP run to highlight ---
# We focus on this noise multiplier for line plots and confusion matrices
TARGET_NOISE_MULTIPLIER = 0.5 
NOISE_VALUES = [0.25, 0.5, 0.75, 1.0] # Must match the batch script

# --- Dependency check ---
try:
    import tabulate
except ImportError:
    print("FATAL ERROR: Missing optional dependency 'tabulate'.")
    print("Please install it using: pip install tabulate")
    sys.exit(1)

# --- Aesthetics Configuration ---
plt.rcParams.update({
    'font.family': 'serif',
    'font.size': 12,
    'axes.titlesize': 16,
    'axes.labelsize': 14,
    'legend.fontsize': 11,
    'figure.dpi': 300,
    'savefig.dpi': 300
})

CUSTOM_COLORS = ['#4C72B0', '#55A868', '#C44E52', '#8172B3']
FINAL_METRICS = ['Loss', 'Accuracy', 'Recall', 'Precision', 'F1-Score', 'AUC']

# --- Static Baseline Data ---
STATIC_BENCHMARK_DATA = {
    "Scenario": ["1. No Balance (Baseline)"],
    "Loss": [0.0175],
    "Accuracy": [0.9800],
    "Recall": [0.5000],
    "Precision": [1.0000],
    "F1-Score": [0.6667],
    "AUC": [1.0000],
}

# --- Load Final Evaluation Data (MODIFIED) ---
def load_single_scenario_eval(filepath, scenario_name, scenario_number, sigma=None):
    scenario_label = f"{scenario_number}. {scenario_name}"
    if sigma is not None:
         # Use LaTeX for sigma in plot labels/table
         scenario_label += f" ($\sigma$={sigma})" 
         
    if not os.path.exists(filepath):
        print(f"Warning: Final evaluation file '{filepath}' not found for {scenario_label}.")
        return pd.DataFrame([{**{'Scenario': scenario_label}, **{m: np.nan for m in FINAL_METRICS}}])
    
    try:
        df = pd.read_csv(filepath)
        def standardize_metric_name(name):
            if name.upper() == 'AUC': return 'AUC'
            if name.lower() == 'f1-score': return 'F1-Score'
            return name.title()
        df['Metric'] = df['Metric'].apply(standardize_metric_name)
        df.set_index('Metric', inplace=True)
        data = df['Value'].to_dict()
        return pd.DataFrame([{**{'Scenario': scenario_label}, **{m: data.get(m, np.nan) for m in FINAL_METRICS}}])
    except Exception as e:
        print(f"❌ Error loading {filepath}: {e}")
        return pd.DataFrame([{**{'Scenario': scenario_label}, **{m: np.nan for m in FINAL_METRICS}}])

def load_final_eval_data():
    df_static = pd.DataFrame(STATIC_BENCHMARK_DATA)
    
    # --- SCENARIO 2: Non-DP (Matches eval_globol.py) ---
    df_non_dp = load_single_scenario_eval('final_non_dp_eval.csv', 'Weights (Non-DP)', 2)
    
    # --- SCENARIO 3/4: All DP runs ---
    dp_runs = []
    for sigma in NOISE_VALUES:
        # Matches eval_globol.py output: final_dp_eval_sigma_{sigma}.csv
        filepath = f'final_dp_eval_sigma_{sigma}.csv' 
        df_dp = load_single_scenario_eval(filepath, 'DP + Weights', 3, sigma=sigma)
        
        # Adjust scenario label for the target sigma to stand out in the bar chart
        if sigma == TARGET_NOISE_MULTIPLIER:
            df_dp['Scenario'] = f"3. DP + Weights (Target $\sigma$={TARGET_NOISE_MULTIPLIER})"
            
        dp_runs.append(df_dp)
        
    return pd.concat([df_static, df_non_dp] + dp_runs, ignore_index=True)

eval_df = load_final_eval_data()

# --- Load History Data (MODIFIED) ---
def load_history_data(filepath, name):
    try:
        if os.path.exists(filepath):
            df = pd.read_csv(filepath)
            df.columns = [col.lower().replace('-', '').replace('score', '') for col in df.columns]
            print(f"✅ Loaded history data from {filepath}")
            return df
        else:
            print(f"Warning: {filepath} ({name}) not found.")
            return None
    except Exception as e:
        print(f"❌ Error loading history file '{filepath}': {e}")
        return None

# Use the specific filenames generated by the batch script and sim_og.py
history_base = load_history_data("simulation_metrics.csv", "Non-DP")
history_dp_path = f"simulation_metrics_dp_sigma_{TARGET_NOISE_MULTIPLIER}.csv"
history_dp = load_history_data(history_dp_path, f"DP ($\sigma$={TARGET_NOISE_MULTIPLIER})")

# --- Plotting Functions (Updated titles/filenames) ---
def plot_confusion_matrix(y_true, y_pred, title, filename):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5), dpi=300)
    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues',
                xticklabels=['Benign (0)', 'Malicious (1)'],
                yticklabels=['Benign (0)', 'Malicious (1)'])
    plt.title(title, fontweight='bold', pad=15)
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')
    plt.tight_layout()
    plt.savefig(filename, bbox_inches='tight')
    plt.close()
    print(f"✅ Saved confusion matrix: {filename}")

def plot_history(history_base, history_dp, metric):
    """Plots history for Non-DP and the TARGET DP run."""
    metric_lower = metric.lower().replace('-', '').replace('score', '')
    plt.figure(figsize=(7, 5), dpi=300)
    
    label_dp = f"3. DP + Weights ($\sigma$={TARGET_NOISE_MULTIPLIER})"
    
    if history_base is not None and metric_lower in history_base.columns:
        plt.plot(history_base['round'], history_base[metric_lower], label='2. Weights (Non-DP)', color=CUSTOM_COLORS[0], linewidth=1.8)
    if history_dp is not None and metric_lower in history_dp.columns:
        plt.plot(history_dp['round'], history_dp[metric_lower], label=label_dp, color=CUSTOM_COLORS[1], linewidth=1.8)
        
    plt.title(f'{metric} vs. Federated Learning Round', fontweight='bold')
    plt.xlabel('Federated Learning Round')
    plt.ylabel(metric)
    plt.legend(frameon=False, loc='best')
    plt.grid(True, linestyle=':', alpha=0.4)
    plt.tight_layout()
    # Save with sigma in the filename
    plt.savefig(f'{metric_lower}_history_sigma_{TARGET_NOISE_MULTIPLIER}.png', bbox_inches='tight') 
    plt.close()
    print(f"✅ Saved: {metric_lower}_history_sigma_{TARGET_NOISE_MULTIPLIER}.png")

def plot_final_comparison(df, metrics_to_plot):
    """Plots comparison including all DP runs."""
    # Find all unique scenarios for plotting
    scenarios = df['Scenario'].unique()
    df_clean = df[df['Scenario'].isin(scenarios)].dropna(subset=metrics_to_plot, how='all')
    if df_clean.empty:
        print("❌ Missing data for final comparison plot.")
        return
    
    # Adjust Scenario names for cleaner X-axis labels (removing the number prefix)
    clean_labels = [s.split('. ')[-1] for s in df_clean['Scenario']]
    
    X = np.arange(len(df_clean['Scenario']))
    bar_width = 0.14
    spacing = bar_width * len(metrics_to_plot)
    plt.figure(figsize=(12, 6), dpi=300) # Increased figure width
    
    for i, metric in enumerate(metrics_to_plot):
        bars = plt.bar(X + i * bar_width, df_clean[metric], width=bar_width, label=metric, color=CUSTOM_COLORS[i % len(CUSTOM_COLORS)], alpha=0.9)
        for bar in bars:
            height = bar.get_height()
            if not np.isnan(height):
                plt.text(bar.get_x() + bar.get_width() / 2, height + 0.015, f"{height:.2f}", ha='center', va='bottom', fontsize=8)
                
    plt.title('Final Global Evaluation Across All Scenarios', fontweight='bold', pad=15)
    plt.ylabel('Score (Higher is Better)')
    plt.xticks(X + spacing / 2 - bar_width / 2, clean_labels, ha='center', rotation=15)
    plt.ylim(0, 1.15)
    plt.yticks(np.arange(0, 1.1, 0.1))
    plt.grid(axis='y', linestyle='--', alpha=0.3)
    plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=len(metrics_to_plot), frameon=False)
    plt.tight_layout(rect=[0, 0.05, 1, 1])
    plt.savefig('final_evaluation_comparison_barchart_all_sigmas.png', bbox_inches='tight')
    plt.close()
    print("✅ Saved: final_evaluation_comparison_barchart_all_sigmas.png")

# --- Markdown Table Generation ---
def save_final_results_md(df, filename="final_evaluation_all_sigmas.md"):
    """Saves the final evaluation results DataFrame as a Markdown table."""
    try:
        df_md = df.copy()
        for col in df_md.columns:
            if col != "Scenario":
                df_md[col] = df_md[col].apply(lambda x: f"{x:.4f}" if pd.notnull(x) else "NaN")
        md_table = df_md.to_markdown(index=False)
        with open(filename, "w") as f:
            f.write(md_table)
        print(f"✅ Saved final evaluation results as Markdown: {filename}")
    except Exception as e:
        print(f"❌ Error saving Markdown table: {e}")

# --- Execution ---
if __name__ == "__main__":
    
    print(f"\n--- Generating History Plots (Focus on $\sigma$={TARGET_NOISE_MULTIPLIER}) ---")
    plot_history(history_base, history_dp, 'Loss')
    plot_history(history_base, history_dp, 'F1-Score')

    print("\n--- Generating Final Comparison Chart (All Sigmas) ---")
    metrics = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'AUC']
    plot_final_comparison(eval_df, metrics)

    print(f"\n--- Generating Confusion Matrices (Focus on $\sigma$={TARGET_NOISE_MULTIPLIER}) ---")
    try:
        # Scenario 2: Non-DP Confusion Matrix - Uses 'predictions_non_dp.csv'
        non_dp_preds_path = 'predictions_non_dp.csv'
        df_non_dp_preds = pd.read_csv(non_dp_preds_path)
        true_labels = df_non_dp_preds['true_label']
        preds_non_dp = df_non_dp_preds['predicted_label']
        plot_confusion_matrix(true_labels, preds_non_dp, 
                              'Confusion Matrix: Scenario 2 (Weights)', 
                              'confusion_matrix_non_dp.png')
        
        # Scenario 3: TARGET DP Confusion Matrix - Uses 'predictions_dp_sigma_0.5.csv'
        dp_preds_path = f'predictions_dp_sigma_{TARGET_NOISE_MULTIPLIER}.csv'
        df_dp_preds = pd.read_csv(dp_preds_path)
        # ASSUMPTION: The 'true_label' in all prediction files is identical (from one test set)
        preds_dp = df_dp_preds['predicted_label'] 
        plot_confusion_matrix(true_labels, preds_dp,
                              f'Confusion Matrix: Scenario 3 (DP + Weights, $\sigma$={TARGET_NOISE_MULTIPLIER})',
                              f'confusion_matrix_dp_sigma_{TARGET_NOISE_MULTIPLIER}.png')
                              
        # Scenario 1: Baseline Confusion Matrix (Simulated)
        baseline_preds = np.zeros_like(true_labels)
        plot_confusion_matrix(true_labels, baseline_preds,
                              'Confusion Matrix: Scenario 1 (Baseline - Simulated)',
                              'confusion_matrix_baseline.png')
                              
    except FileNotFoundError as e:
        print(f"❌ Prediction file not found: {e}. Please ensure you have run 'evaluate_global.py' for all necessary scenarios.")
    except Exception as e:
        print(f"❌ An error occurred while generating confusion matrices: {e}")

    print("\n--- Generating Markdown Table (All Sigmas) ---")
    save_final_results_md(eval_df, "final_evaluation_all_sigmas.md")

    print("\nAll visualizations generated successfully for publication.")